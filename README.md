# Adverse Drive

The goal of this project is to attack end-to-end self-driving models using physically realizable adversaries.

<center>
<img src="media/concept_overview.png"/>
</center>

<center>
Concept Overview
</center>

<center>
<img src="media/hijack_final.gif"/>
</center>
<center>
Hijacking Attack Objective
</center>

### Pre-requisites

- Ubuntu 16.04
- Dedicated GPU with relevant CUDA drivers

## Quick Start
1. Clone the AdverseDrive repo

```
git clone https://github.com/xz-group/AdverseDrive
```

2. Export Carla paths to `PYTHONPATH`

```
source export_paths.sh
```

3. Install the required Python packages

```
pip3 install -r requirements.txt # requires check
```

4. Download the modified version of the Carla simulator[1]

```
Box or Google Drive Link
```

5. Run the Carla simulator on a terminal

```
./Carla.sh -windowed -ResX=800 -ResY=600
```
This starts Carla as a server on port 2000. Give it about 10-30 seconds to start up depending on your system.

6. On a new terminal, start a python HTTP server. This allows the Carla simulator to read the generated attack images and load it onto Carla

```
sh run_adv_server.sh
```
Note: This requires port 8000 to be free.

7. On another new terminal, run the infraction objective python script

```
python3 run_infraction_experiments.py
```
Note: the Jupyter notebook version of this script, called `run_infraction_experiments.ipynb` describes each step in detail. It is recommended to use that while starting out with this repository. Use `jupyter notebook` to start a jupyter server in this directory.

## Process

1. The above steps, sets up an experiment defined by the experiment parameters in `config/infraction_parameters.json`, including the Carla Town being used, the task (straight, turn-left, turn-right), different scenes, the port number being used by Carla and Bayesian optimizer[3] parameters.
2. Runs the `baseline scenario` where the Carla Imitation Learning[2] (IL) agent drives a vehicle from point A to point B as defined by the experiment scene and task. It returns a metric from the run (eg: sum of infraction for each frame). The baseline scenario is when there is no attack.
3. The Bayesian Optimizer suggests parameters for the attack, based on the returned metric (which serves as the objective function that we are trying to maximize), the attack is generated by `adversary_generator.py` and placed in `adversary/adversary_{town_name}.png`.
4. Carla reads the adversary image over the HTTP server and places in on pre-determined locations within the road.
5. The IL model again runs through this `attack scenario` and returns a metric.
6. Steps 3-5 are repeated for a set number of experiments, in which successful attacks would be found.


### Docker Method
1. Clone the AdverseDrive repo

```
git clone https://github.com/xz-group/AdverseDrive
```

2. Pull the modified version of the Carla simulator:

```
docker pull xzgroup/carla:latest
```

3. Pull the `AdverseDrive` docker containing all the prerequisite packages for running experiments (also server-friendly)

```
docker pull xzgroup/adversedrive:latest
```

4. Run the our dockerized Carla simulator on a terminal

```
sh run_carla_docker.sh
```
This starts Carla as a server on port 2000. Give it about 10-30 seconds to start up depending on your system.

6. On a new terminal, start a python HTTP server. This allows the Carla simulator to read the generated attack images and load it onto Carla

```
sh run_adv_server.sh
```
Note: This requires port 8000 to be free.

7. On another new terminal, run the `xzgroup/adversedrive` docker

```
sh run_docker.sh
```

8. Run the infraction objective python script

```
python3 run_infraction_experiments.py
```

## References
1. Carla Simulator: [https://github.com/carla-simulator/carla](https://github.com/carla-simulator/carla)
2. Imitation Learning: [https://github.com/carla-simulator/imitation-learning](https://github.com/carla-simulator/imitation-learning)
3. Bayesian Optimization: [https://github.com/fmfn/BayesianOptimization](https://github.com/fmfn/BayesianOptimization)

## Citation
If you use our work, kindly cite us using the following:
```
@article{place_holder,
  author    = {Adith Boloor, Karthik Garimella, Xin He,
               Christopher D. Gill, Yevgeniy Vorobeychik and Xuan Zhang},
  title     = {Attacking Vision based Perception in End-to-end
              Autonomous Driving Models},
  journal   = {placeholder},
  volume    = {placeholder},
  year      = {2019},
  url       = {http://arxiv.org/abs/placeholder},
  archivePrefix = {arXiv},
  eprint    = {placeholder},
}
```
